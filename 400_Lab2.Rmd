---
title: "400 Lab Assignment 2"
author: "Chuan Du (Sophie)"
date: "11/8/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load data
redwine = read.table("redwine.txt", header = TRUE); head(redwine, 5)
```


### Problem 1

```{r}
#remove NA
RS_avg = mean(redwine$RS, na.rm = TRUE); RS_avg
```

```{r}
SD_avg = mean(redwine$SD, na.rm = TRUE); SD_avg
```

**Answer: ** $avg(RS) = 2.537952$ and $avg(SD) = 46.29836$.


### Problem 2

```{r}
#find which obs in SD are NA
na_index = which(is.na(redwine$SD))
#remove these NA in SD
SD = na.omit(redwine$SD)
#remove FS obs with these indices
FS = redwine$FS[-na_index]
#fit the model
mod2 = lm(SD ~ FS)
mod2$coefficients
```

**Answer: ** The coefficients of the regression model is **13.185505** and **2.086077**.


### Problem 3

```{r}
FS.impute = redwine$FS[na_index] 
SD.impute = coefficients(mod2)[1] + coefficients(mod2)[2] * FS.impute
redwine$SD[na_index] = SD.impute
mean(redwine$SD)
```

**Answer: ** The average of SD after the imputation is **46.30182**.


### Problem 4

```{r}
#define avg value imputation
avg.imp = function(x, avg){
  missing = is.na(x)
  imputed = x
  imputed[missing] = avg
  return(imputed)
}

#apply the method to RS
RS_imp = avg.imp(redwine$RS, RS_avg)
mean(RS_imp)
```

**Answer: ** The average of RS after the imputation is **2.537952**,


### Problem 5

```{r}
#fill in na of RS by avg imputation
redwine$RS = RS_imp
```

```{r}
redwinemodel = lm(QA ~ ., data = redwine)
redwinemodel$coefficients
```


### Problem 6

```{r}
summary(redwinemodel)
```

**Answer: ** Based on the model summary, we could see that **PH** is a *non-significant* predictor and with the *largest p-value*, so **PH** is least likely to be related to QA.


### Problem 7

```{r}
CV_i = function(n, K){
  #n is sample size, k is number of folds
  #returns k-len lst of indices for each part
  m = floor(n/K) #approximate size of each part
  r = n - m*K
  I = sample(n, n) #random reordering of the indices
  Ind = list() #index for all k parts
  length(Ind) = K
  for (k in 1:K){
    if (k <= r)
      kpart = ((m+1)*(k-1)+1):((m+1)*k)
    else
      kpart = ((m+1)*r+m*(k-r-1)+1):((m+1)*r+m*(k-r))
    Ind[[k]] = I[kpart]  #indices for kth part of data
  }
  Ind
}
```

```{r}
Nrep = 20 #repeat CV 20 times
K =  5 #5-fold cv
n = nrow(redwine)
y = redwine$QA
SSE = c()
for (j in 1:Nrep){
  Ind = CV_i(n, K)
  yhat = y
  for (k in 1:K){
    out = lm(QA ~., data = redwine[-Ind[[k]], ])
    yhat[Ind[[k]]] = as.numeric(predict(out, redwine[Ind[[k]], ]))
  }
  SSE = c(SSE, sum((y - yhat)^2))
}
SSE
```

```{r}
mean(SSE)
```


### Problem 8

```{r}
mu = mean(redwine$PH); mu
```

```{r}
sigma = sd(redwine$PH); sigma
```

```{r}
redwine2 = subset(redwine, redwine$PH >= mu-3*sigma & redwine$PH <= mu+3*sigma)
dim(redwine2)
```

```{r}
dim(redwine)[1] - dim(redwine2)[1]
```

**Answer: ** For the selected attribute *PH*, the average $\mu = 3.306202$, the standard deviation $\sigma = 0.3924948$. After removing observations that is outside the range $[\mu - 3\sigma, \mu + 3\sigma]$, we have the new dataset with dimension $1580*12$, and by comparing with the original dataset, we have removed **19** observations.


### Problem 9

```{r}
redwinemodel2 = lm(QA ~ ., data = redwine2)
summary(redwinemodel2)
```

**Answer: ** By comparing the models, we could see that **the new model is better**, since $R^2$ increases, $R^2_{adj}$ increases and F-statistics increases after we remove outliers and impute missing values. *VA, CH, SD, SU, AL* are the 5 attributes that are most likely to be related to QA based on p-values.